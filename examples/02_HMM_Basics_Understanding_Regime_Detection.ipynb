{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM Basics: Understanding Regime Detection\n",
    "\n",
    "**From Log Returns to Market Regimes Using Hidden Markov Models**\n",
    "\n",
    "In the previous notebook, we proved that log returns are the correct transformation for financial time series.\n",
    "\n",
    "Now we'll use those log returns to detect market regimes using **Hidden Markov Models (HMMs)**.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. What HMMs are and how they work\n",
    "2. What parameters HMMs learn from data\n",
    "3. How to interpret HMM states (WITHOUT forcing Bull/Bear labels)\n",
    "4. How to choose the right number of states\n",
    "5. How to visualize regime sequences\n",
    "\n",
    "## Important Note\n",
    "\n",
    "**This notebook uses minimal code** to understand HMM mechanics.\n",
    "\n",
    "**Next notebook** shows the full library pipeline with analysis tools and best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Add parent to path\nsys.path.insert(0, str(Path().absolute().parent))\n\n# Hidden Regime imports - minimal\nfrom hidden_regime.data import FinancialDataLoader\nfrom hidden_regime.observations import FinancialObservationGenerator\nfrom hidden_regime.models import HiddenMarkovModel\nfrom hidden_regime.config import FinancialDataConfig, FinancialObservationConfig, HMMConfig\nfrom hidden_regime.utils import log_return_to_percent_change\n\n# Plotting\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette(\"husl\")\n\nprint(\"Imports complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a Hidden Markov Model?\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "An HMM assumes the market operates in **hidden states** (regimes) that we cannot directly observe. Each regime has:\n",
    "- **Different return characteristics** (mean and volatility)\n",
    "- **Persistence** (tendency to stay in the same regime)\n",
    "- **Transition probabilities** (likelihood of switching to other regimes)\n",
    "\n",
    "### The Three Components\n",
    "\n",
    "1. **Hidden States** (`S₁, S₂, ..., Sₜ`): The unobserved regime sequence\n",
    "2. **Observations** (`O₁, O₂, ..., Oₜ`): The log returns we can see\n",
    "3. **Parameters**:\n",
    "   - **Transition Matrix** `A`: `P(regime_{t+1} = j | regime_t = i)`\n",
    "   - **Emission Means** `μ`: Average log return for each regime\n",
    "   - **Emission Stds** `σ`: Volatility for each regime\n",
    "   - **Initial Probs** `π`: Starting regime distribution\n",
    "\n",
    "### What HMMs Do\n",
    "\n",
    "Given observed log returns, HMMs **simultaneously**:\n",
    "- Infer the most likely regime sequence (states)\n",
    "- Learn the transition probabilities (how regimes switch)\n",
    "- Learn emission parameters (return characteristics of each regime)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Create Observations\n",
    "\n",
    "We'll use SPY (S&P 500 ETF) with 2 years of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data loader\n",
    "data_config = FinancialDataConfig(\n",
    "    ticker='SPY',\n",
    "    end_date=datetime.now(),\n",
    "    num_samples=504  # ~2 years of trading days\n",
    ")\n",
    "\n",
    "# Load data\n",
    "loader = FinancialDataLoader(data_config)\n",
    "df = loader.load_data()\n",
    "\n",
    "print(f\"Loaded {len(df)} days of SPY data\")\n",
    "print(f\"Date range: {df.index[0].date()} to {df.index[-1].date()}\")\n",
    "print(f\"\\nData columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create observation generator\nobs_config = FinancialObservationConfig(\n    generators=[\"log_return\"],  # Just log returns\n    price_column=\"close\",\n    normalize_features=False  # Keep raw log returns\n)\n\nobs_generator = FinancialObservationGenerator(obs_config)\nobservations_df = obs_generator.update(df)\n\n# For HMM training, we need just the log_return column as a DataFrame\nobservations = observations_df[['log_return']]\n\nprint(f\"Generated {len(observations)} observations\")\nprint(f\"Observation shape: {observations.shape}\")\nprint(f\"\\nObservations are log returns:\")\nprint(f\"  Mean: {observations.values.mean():.6f}\")\nprint(f\"  Std:  {observations.values.std():.6f}\")\nprint(f\"  Min:  {observations.values.min():.6f}\")\nprint(f\"  Max:  {observations.values.max():.6f}\")\n\n# Convert a few examples to percentage for interpretation\nprint(f\"\\nExample log returns converted to percentages:\")\nfor i in range(min(5, len(observations))):\n    log_ret = observations.iloc[i, 0]\n    pct = log_return_to_percent_change(log_ret) * 100\n    print(f\"  Day {i+1}: log_return={log_ret:.6f} → {pct:+.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the HMM\n",
    "\n",
    "Now we'll train a 3-state HMM on these log returns. The model will learn:\n",
    "- Which regime the market was in each day\n",
    "- The transition probabilities between regimes\n",
    "- The mean return and volatility for each regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure HMM with 3 states\nhmm_config = HMMConfig(\n    n_states=3,\n    max_iterations=100,  # Maximum training iterations\n    tolerance=1e-4,      # Convergence tolerance\n    random_seed=42\n)\n\n# Create and train model\nprint(\"Training 3-state HMM...\")\nhmm = HiddenMarkovModel(hmm_config)\nhmm.fit(observations)\n\nprint(f\"Training complete!\")\nprint(f\"  Converged: {hmm.training_history_['converged']}\")\nprint(f\"  Iterations: {hmm.training_history_['iterations']}\")\nprint(f\"  Final log-likelihood: {hmm.score(observations):.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Learned Parameters\n",
    "\n",
    "Let's see what the HMM learned about the three regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract learned parameters\ntransition_matrix = hmm.transition_matrix_\nemission_means = hmm.emission_means_\nemission_stds = hmm.emission_stds_\nstart_probs = hmm.initial_probs_\n\nprint(\"=\" * 80)\nprint(\"LEARNED PARAMETERS\")\nprint(\"=\" * 80)\n\nprint(\"\\n1. EMISSION PARAMETERS (Return Characteristics)\")\nprint(\"-\" * 80)\nprint(f\"{'State':<10} {'Mean (log)':<15} {'Mean (%/day)':<15} {'Std (log)':<15} {'Vol (%/day)':<15}\")\nprint(\"-\" * 80)\n\n# Sort by mean for easier interpretation\nsorted_idx = np.argsort(emission_means)\n\nfor idx in sorted_idx:\n    log_mean = emission_means[idx]\n    pct_mean = log_return_to_percent_change(log_mean) * 100\n    log_std = emission_stds[idx]\n    pct_std = log_std * 100  # Approximation valid for small returns\n    \n    print(f\"State {idx:<4} {log_mean:>14.6f} {pct_mean:>14.2f}% {log_std:>14.6f} {pct_std:>14.2f}%\")\n\nprint(\"\\n2. TRANSITION MATRIX (Regime Switching Probabilities)\")\nprint(\"-\" * 80)\nprint(\"         To State 0  To State 1  To State 2\")\nprint(\"-\" * 80)\nfor i in range(3):\n    probs = \" \".join([f\"{transition_matrix[i,j]:>11.3f}\" for j in range(3)])\n    print(f\"From {i}:  {probs}\")\n\nprint(\"\\n3. STARTING PROBABILITIES\")\nprint(\"-\" * 80)\nfor i in range(3):\n    print(f\"State {i}: {start_probs[i]:.3f}\")\n\nprint(\"\\n\" + \"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predict Regime Sequence\n",
    "\n",
    "Now let's use the Viterbi algorithm to find the most likely sequence of regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Predict states using Viterbi algorithm\npredictions_df = hmm.predict(observations)\nstates = predictions_df['predicted_state'].values\n\nprint(f\"Predicted {len(states)} regime states\")\nprint(f\"\\nState distribution:\")\nfor state in range(3):\n    count = (states == state).sum()\n    pct = count / len(states) * 100\n    mean_ret = emission_means[state]\n    mean_ret_pct = log_return_to_percent_change(mean_ret) * 100\n    print(f\"  State {state}: {count:>4} days ({pct:>5.1f}%) - Avg return: {mean_ret_pct:+.2f}%/day\")\n\n# Calculate regime durations\ndef get_regime_durations(states):\n    \"\"\"Calculate how long each regime lasted\"\"\"\n    durations = []\n    current_state = states[0]\n    current_duration = 1\n    \n    for i in range(1, len(states)):\n        if states[i] == current_state:\n            current_duration += 1\n        else:\n            durations.append((current_state, current_duration))\n            current_state = states[i]\n            current_duration = 1\n    \n    durations.append((current_state, current_duration))\n    return durations\n\ndurations = get_regime_durations(states)\nprint(f\"\\nRegime switches: {len(durations)} total regime periods\")\nprint(f\"\\nAverage duration by state:\")\nfor state in range(3):\n    state_durations = [d for s, d in durations if s == state]\n    if state_durations:\n        avg_dur = np.mean(state_durations)\n        print(f\"  State {state}: {avg_dur:.1f} days\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Regime Sequence\n",
    "\n",
    "Let's see how the detected regimes align with price movements and returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create visualization\nfig, axes = plt.subplots(3, 1, figsize=(16, 12), sharex=True)\n\n# Define colors for each state (sorted by mean return)\nstate_colors = {sorted_idx[0]: 'red', sorted_idx[1]: 'gray', sorted_idx[2]: 'green'}\nstate_names = {sorted_idx[0]: 'Negative', sorted_idx[1]: 'Neutral', sorted_idx[2]: 'Positive'}\n\n# 1. Price with regime shading\nax = axes[0]\nprices = df['close'].values\ndates = df.index\n\nax.plot(dates, prices, linewidth=1.5, color='black', zorder=2)\nax.set_ylabel('Price ($)', fontsize=12)\nax.set_title('SPY Price with Detected Regimes', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\n# Shade background by regime\nfor i in range(len(states)):\n    state = states[i]\n    ax.axvspan(dates[i], dates[min(i+1, len(dates)-1)], \n               alpha=0.2, color=state_colors[state], zorder=1)\n\n# Add legend\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=state_colors[s], alpha=0.2, \n                         label=f'State {s} ({state_names[s]})') \n                   for s in sorted_idx]\nax.legend(handles=legend_elements, loc='upper left')\n\n# 2. Log returns with regime shading\nax = axes[1]\nlog_returns = observations.flatten()\nax.plot(dates[1:], log_returns, linewidth=0.8, color='blue', alpha=0.7)\nax.axhline(y=0, color='black', linestyle='--', linewidth=1)\nax.set_ylabel('Log Return', fontsize=12)\nax.set_title('Log Returns with Detected Regimes', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\n\n# Shade background\nfor i in range(len(states)):\n    state = states[i]\n    ax.axvspan(dates[i], dates[min(i+1, len(dates)-1)], \n               alpha=0.2, color=state_colors[state], zorder=1)\n\n# 3. Regime sequence as bar plot\nax = axes[2]\nregime_bars = ax.bar(dates[1:], np.ones(len(states)), width=1, \n                      color=[state_colors[s] for s in states], \n                      edgecolor='none', alpha=0.6)\nax.set_ylabel('Regime', fontsize=12)\nax.set_xlabel('Date', fontsize=12)\nax.set_title('Regime Sequence', fontsize=14, fontweight='bold')\nax.set_ylim(0, 1.5)\nax.set_yticks([])\nax.grid(True, alpha=0.3, axis='x')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nNote: Notice how regimes cluster periods of similar return behavior\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Choosing the Number of States\n",
    "\n",
    "How do we decide between 2, 3, 4, or more states? Let's compare different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train models with different numbers of states\nprint(\"Comparing models with different numbers of states...\")\nprint(\"=\" * 80)\n\nresults = []\n\nfor n_states in [2, 3, 4, 5]:\n    config = HMMConfig(n_states=n_states, max_iterations=100, random_seed=42)\n    model = HiddenMarkovModel(config)\n    model.fit(observations)\n    \n    # Calculate metrics\n    log_likelihood = model.score(observations)\n    n_params = n_states**2 + 2*n_states  # Transitions + means + stds\n    aic = -2 * log_likelihood + 2 * n_params\n    bic = -2 * log_likelihood + n_params * np.log(len(observations))\n    \n    results.append({\n        'n_states': n_states,\n        'log_likelihood': log_likelihood,\n        'aic': aic,\n        'bic': bic,\n        'n_params': n_params\n    })\n    \n    print(f\"\\n{n_states} states:\")\n    print(f\"  Log-likelihood: {log_likelihood:>10.2f}\")\n    print(f\"  AIC:            {aic:>10.2f} (lower is better)\")\n    print(f\"  BIC:            {bic:>10.2f} (lower is better)\")\n    print(f\"  Parameters:     {n_params:>10}\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Find best models\nresults_df = pd.DataFrame(results)\nbest_aic = results_df.loc[results_df['aic'].idxmin()]\nbest_bic = results_df.loc[results_df['bic'].idxmin()]\n\nprint(f\"\\nBest by AIC: {int(best_aic['n_states'])} states (AIC={best_aic['aic']:.2f})\")\nprint(f\"Best by BIC: {int(best_bic['n_states'])} states (BIC={best_bic['bic']:.2f})\")\n\nprint(\"\\nNote: BIC often prefers simpler models (fewer states)\")\nprint(\"   AIC may prefer more complex models that fit better\")\nprint(\"   For trading, 3-4 states usually provides good interpretability\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Log-likelihood\n",
    "ax = axes[0]\n",
    "ax.plot(results_df['n_states'], results_df['log_likelihood'], marker='o', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Number of States', fontsize=12)\n",
    "ax.set_ylabel('Log-Likelihood', fontsize=12)\n",
    "ax.set_title('Model Fit (Higher is Better)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks([2, 3, 4, 5])\n",
    "\n",
    "# Plot 2: AIC\n",
    "ax = axes[1]\n",
    "ax.plot(results_df['n_states'], results_df['aic'], marker='o', linewidth=2, markersize=8, color='orange')\n",
    "best_aic_idx = results_df['aic'].idxmin()\n",
    "ax.scatter(results_df.loc[best_aic_idx, 'n_states'], \n",
    "           results_df.loc[best_aic_idx, 'aic'], \n",
    "           color='red', s=150, zorder=5, label='Best')\n",
    "ax.set_xlabel('Number of States', fontsize=12)\n",
    "ax.set_ylabel('AIC', fontsize=12)\n",
    "ax.set_title('AIC (Lower is Better)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_xticks([2, 3, 4, 5])\n",
    "\n",
    "# Plot 3: BIC\n",
    "ax = axes[2]\n",
    "ax.plot(results_df['n_states'], results_df['bic'], marker='o', linewidth=2, markersize=8, color='green')\n",
    "best_bic_idx = results_df['bic'].idxmin()\n",
    "ax.scatter(results_df.loc[best_bic_idx, 'n_states'], \n",
    "           results_df.loc[best_bic_idx, 'bic'], \n",
    "           color='red', s=150, zorder=5, label='Best')\n",
    "ax.set_xlabel('Number of States', fontsize=12)\n",
    "ax.set_ylabel('BIC', fontsize=12)\n",
    "ax.set_title('BIC (Lower is Better)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_xticks([2, 3, 4, 5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **HMM Components**:\n",
    "   - Hidden states represent market regimes\n",
    "   - Transition matrix captures regime switching behavior\n",
    "   - Emission parameters (μ, σ) define return characteristics\n",
    "\n",
    "2. **Training Process**:\n",
    "   - Baum-Welch algorithm learns parameters from data\n",
    "   - Viterbi algorithm finds most likely state sequence\n",
    "   - Converges to local optimum (results may vary)\n",
    "\n",
    "3. **Interpretation**:\n",
    "   - States are numbered arbitrarily (0, 1, 2)\n",
    "   - Must examine emission means to understand regime types\n",
    "   - DO NOT force \"Bear/Sideways/Bull\" labels prematurely\n",
    "\n",
    "4. **Model Selection**:\n",
    "   - Use AIC/BIC to compare different numbers of states\n",
    "   - Balance fit quality vs. model complexity\n",
    "   - 3-4 states often optimal for interpretability\n",
    "\n",
    "### Important Warnings\n",
    "\n",
    "⚠️ **DO NOT**:\n",
    "- Sort states by index and call them Bear/Sideways/Bull\n",
    "- Assume state 0 is always negative returns\n",
    "- Use HMMs on non-stationary data (prices)\n",
    "\n",
    "✓ **DO**:\n",
    "- Use log returns (stationary observations)\n",
    "- Examine learned parameters before labeling\n",
    "- Compare multiple model complexities\n",
    "- Validate regime assignments make sense\n",
    "\n",
    "---\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Notebook 3** will show the full pipeline:\n",
    "- Regime labeling and analysis tools\n",
    "- Advanced diagnostics and quality metrics\n",
    "- Portfolio applications\n",
    "- Best practices for production use\n",
    "\n",
    "---\n",
    "\n",
    "**Key Point**: HMMs are a **data-driven** approach. Let the model tell you what the regimes are - don't force your preconceptions onto the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}