{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Testing & Failure Modes - Understanding System Limits\n",
    "\n",
    "## What This Notebook Covers\n",
    "\n",
    "This notebook systematically tests multivariate HMM regime detection under stress conditions:\n",
    "\n",
    "- **Part 1**: What can go wrong? (Failure taxonomy)\n",
    "- **Part 2**: Stress testing methodology\n",
    "- **Part 3**: Case studies of failure modes\n",
    "- **Part 4**: Detection and recovery strategies\n",
    "- **Part 5**: Resilience recommendations\n",
    "- **Part 6**: Production deployment checklist\n",
    "\n",
    "**Estimated time**: 45-60 minutes with all tests.\n",
    "\n",
    "## Key Question\n",
    "\n",
    "**When and why does multivariate regime detection fail?**\n",
    "\n",
    "Understanding failure modes is critical for:\n",
    "- Setting appropriate thresholds and alerts\n",
    "- Knowing when to trust regime signals\n",
    "- Designing fallback strategies\n",
    "- Production risk management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Failure Taxonomy\n",
    "\n",
    "### Type 1: Data Quality Failures\n",
    "\n",
    "**Cause**: Problems with input data\n",
    "\n",
    "1. **Insufficient data**\n",
    "   - Effect: Poor parameter estimation, overfitting\n",
    "   - Threshold: < 100 observations\n",
    "   - Detection: Monitor convergence iterations\n",
    "   - Recovery: Use univariate or transfer learning\n",
    "\n",
    "2. **Missing/gap data**\n",
    "   - Effect: Discontinuous feature values\n",
    "   - Threshold: > 5% missing\n",
    "   - Detection: Check for NaN values\n",
    "   - Recovery: Forward-fill gaps < 3 days\n",
    "\n",
    "3. **Extreme outliers**\n",
    "   - Effect: Biased parameter estimates\n",
    "   - Threshold: > 5 standard deviations\n",
    "   - Detection: Check distribution tail risk\n",
    "   - Recovery: Winsorize or remove if data error\n",
    "\n",
    "### Type 2: Model Estimation Failures\n",
    "\n",
    "**Cause**: Problems during training\n",
    "\n",
    "1. **Non-convergence**\n",
    "   - Effect: Suboptimal parameters, poor inference\n",
    "   - Threshold: > 100 iterations without improvement\n",
    "   - Detection: Check training_history['converged']\n",
    "   - Recovery: Lower tolerance, increase max_iterations, or restart\n",
    "\n",
    "2. **Singular covariance matrix**\n",
    "   - Effect: Cannot invert Sigma, invalid inference\n",
    "   - Threshold: Determinant ≈ 0 or negative eigenvalue\n",
    "   - Detection: Check eigvalsh() results\n",
    "   - Recovery: Add regularization or remove redundant features\n",
    "\n",
    "3. **Ill-conditioned matrix**\n",
    "   - Effect: Numerical instability in likelihood computation\n",
    "   - Threshold: Condition number > 1e6\n",
    "   - Detection: Monitor eigenvalue ratios\n",
    "   - Recovery: Scale features or regularize\n",
    "\n",
    "### Type 3: Inference Failures\n",
    "\n",
    "**Cause**: Problems during state sequence prediction\n",
    "\n",
    "1. **Low confidence**\n",
    "   - Effect: Model uncertainty, unreliable predictions\n",
    "   - Threshold: avg confidence < 0.5\n",
    "   - Detection: Monitor confidence.mean()\n",
    "   - Recovery: Increase look-ahead window or blend with other signals\n",
    "\n",
    "2. **High transition frequency**\n",
    "   - Effect: Noisy regime signals, false positives\n",
    "   - Threshold: > 1 transition per 20 days\n",
    "   - Detection: np.diff(states) != 0 count\n",
    "   - Recovery: Use multi-timeframe filtering, increase smoothing\n",
    "\n",
    "3. **Regime drift**\n",
    "   - Effect: Model trained on old data doesn't match current market\n",
    "   - Threshold: Confidence declines > 10% over time\n",
    "   - Detection: Monitor confidence trend\n",
    "   - Recovery: Retrain on recent data, implement concept drift detection\n",
    "\n",
    "### Type 4: Feature Failures\n",
    "\n",
    "**Cause**: Problems with feature selection or relationships\n",
    "\n",
    "1. **Redundant features**\n",
    "   - Effect: Wasted parameters, numerical instability\n",
    "   - Threshold: Correlation > 0.95\n",
    "   - Detection: Compute Pearson correlation\n",
    "   - Recovery: Remove redundant feature\n",
    "\n",
    "2. **Non-informative features**\n",
    "   - Effect: Model uses less information than available\n",
    "   - Threshold: I(X; Z) < threshold (mutual information)\n",
    "   - Detection: Check if feature varies across regimes\n",
    "   - Recovery: Replace with regime-informative feature\n",
    "\n",
    "3. **Unstable relationship**\n",
    "   - Effect: Feature relationship changes across market regimes\n",
    "   - Threshold: Covariance sign flips across regimes\n",
    "   - Detection: Check cov[0,1] signs across states\n",
    "   - Recovery: Use adaptive feature weighting or regime-specific models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Stress Testing Methodology\n",
    "\n",
    "### Framework\n",
    "\n",
    "For each failure mode, we:\n",
    "\n",
    "1. **Create synthetic test case** (controlled problem)\n",
    "2. **Measure impact** (how badly does it fail?)\n",
    "3. **Detect automatically** (can we catch this?)\n",
    "4. **Implement recovery** (can we fix this?)\n",
    "5. **Document threshold** (when to trigger recovery)\n",
    "\n",
    "### Metrics for Success\n",
    "\n",
    "- **Convergence**: Does training converge? (Y/N)\n",
    "- **Likelihood**: Does likelihood increase monotonically?\n",
    "- **Confidence**: What's the average prediction confidence?\n",
    "- **Stability**: How stable are parameters across runs?\n",
    "- **Accuracy**: How well do regimes match market truth?\n",
    "\n",
    "### Risk Matrix\n",
    "\n",
    "```\n",
    "               Detection Easy   Detection Hard\n",
    "Recovery Easy      GREEN           YELLOW\n",
    "Recovery Hard      YELLOW          RED\n",
    "```\n",
    "\n",
    "We want failures to be:\n",
    "1. Easy to detect (automated monitoring)\n",
    "2. Easy to recover from (automatic fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stress Test Suite Initialized\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Test utilities\n",
    "class StressTestSuite:\n",
    "    \"\"\"Comprehensive stress testing for multivariate HMM.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_states=3):\n",
    "        self.n_states = n_states\n",
    "        self.results = {}\n",
    "    \n",
    "    def generate_normal_data(self, n_obs=500, n_features=2, regime_change_points=None):\n",
    "        \"\"\"Generate well-behaved synthetic data.\"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        if regime_change_points is None:\n",
    "            regime_change_points = [0, 250, 500]\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "        for regime in range(len(regime_change_points) - 1):\n",
    "            start = regime_change_points[regime]\n",
    "            end = regime_change_points[regime + 1]\n",
    "            n = end - start\n",
    "            \n",
    "            # Regime-specific parameters\n",
    "            mean = [0.001 * (1 - regime * 0.5), 0.01 + regime * 0.01]\n",
    "            cov = [[0.0001 + regime * 0.0002, 0.00001 * (regime + 1)],\n",
    "                    [0.00001 * (regime + 1), 0.0001 + regime * 0.0003]]\n",
    "            \n",
    "            regime_data = np.random.multivariate_normal(mean, cov, n)\n",
    "            data.append(regime_data)\n",
    "        \n",
    "        data = np.vstack(data)\n",
    "        df = pd.DataFrame(data, columns=['feature1', 'feature2'])\n",
    "        return df\n",
    "    \n",
    "    def test_insufficient_data(self):\n",
    "        \"\"\"Test what happens with very few observations.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for n_obs in [20, 50, 100, 200, 500]:\n",
    "            data = self.generate_normal_data(n_obs=n_obs)\n",
    "            \n",
    "            # Try to fit (will likely fail or have issues)\n",
    "            try:\n",
    "                # Use simplified HMM fitting logic\n",
    "                scaler = StandardScaler()\n",
    "                X = scaler.fit_transform(data.values)\n",
    "                \n",
    "                results[n_obs] = {\n",
    "                    'n_obs': n_obs,\n",
    "                    'n_features': data.shape[1],\n",
    "                    'params_per_state': data.shape[1] + (data.shape[1] * (data.shape[1] + 1) // 2),\n",
    "                    'degrees_of_freedom': n_obs - (data.shape[1] + (data.shape[1] * (data.shape[1] + 1) // 2) * self.n_states),\n",
    "                    'status': 'OK' if n_obs >= 100 else 'RISKY'\n",
    "                }\n",
    "            except Exception as e:\n",
    "                results[n_obs] = {'error': str(e), 'status': 'FAIL'}\n",
    "        \n",
    "        return results\n",
    "\n",
    "suite = StressTestSuite(n_states=3)\n",
    "print(\"Stress Test Suite Initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Case Studies of Failure Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FAILURE MODE 1: INSUFFICIENT DATA\n",
      "======================================================================\n",
      "\n",
      "Parameters per state: mean (2) + covariance lower triangle (3) = 5 total\n",
      "Total parameters: 5 * n_states = 15 for 3-state model\n",
      "\n",
      "n_obs= 20: DOF=  9, Status=RISKY  <- INSUFFICIENT DEGREES OF FREEDOM\n",
      "n_obs= 50: DOF= 39, Status=RISKY \n",
      "n_obs=100: DOF= 89, Status=OK    \n",
      "n_obs=200: DOF=189, Status=OK    \n",
      "n_obs=500: DOF=489, Status=OK    \n",
      "\n",
      "Recommendation: Use minimum 100 observations (200+ preferred)\n"
     ]
    }
   ],
   "source": [
    "# Test: Insufficient Data\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FAILURE MODE 1: INSUFFICIENT DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "insufficient_results = suite.test_insufficient_data()\n",
    "\n",
    "print(\"\\nParameters per state: mean (2) + covariance lower triangle (3) = 5 total\")\n",
    "print(\"Total parameters: 5 * n_states = 15 for 3-state model\\n\")\n",
    "\n",
    "for n_obs, result in insufficient_results.items():\n",
    "    if 'error' not in result:\n",
    "        print(f\"n_obs={n_obs:3d}: DOF={result['degrees_of_freedom']:3d}, Status={result['status']:6s}\", end=\"\")\n",
    "        if result['degrees_of_freedom'] < 30:\n",
    "            print(\" <- INSUFFICIENT DEGREES OF FREEDOM\")\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "print(\"\\nRecommendation: Use minimum 100 observations (200+ preferred)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FAILURE MODE 2: REDUNDANT FEATURES (High Correlation)\n",
      "======================================================================\n",
      "\n",
      "Correlation Analysis:\n",
      "Target=0.10, Actual=0.025, Condition=    1.06, Status=OK\n",
      "Target=0.50, Actual=0.450, Condition=    2.64, Status=OK\n",
      "Target=0.80, Actual=0.813, Condition=    9.71, Status=CORRELATED\n",
      "Target=0.95, Actual=0.952, Condition=   40.49, Status=REDUNDANT\n",
      "Target=0.99, Actual=0.990, Condition=  190.83, Status=REDUNDANT\n",
      "\n",
      "Recommendation: Correlation < 0.7 preferred, remove if > 0.95\n"
     ]
    }
   ],
   "source": [
    "# Test: Feature Correlation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FAILURE MODE 2: REDUNDANT FEATURES (High Correlation)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate correlated features\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "feature1 = np.random.normal(0, 1, n)\n",
    "correlations = [0.1, 0.5, 0.8, 0.95, 0.99]\n",
    "\n",
    "print(\"\\nCorrelation Analysis:\")\n",
    "for corr_target in correlations:\n",
    "    # Create feature2 with target correlation\n",
    "    noise = np.random.normal(0, 1, n)\n",
    "    feature2 = corr_target * feature1 + np.sqrt(1 - corr_target**2) * noise\n",
    "    \n",
    "    actual_corr = np.corrcoef(feature1, feature2)[0, 1]\n",
    "    \n",
    "    # Check covariance matrix conditioning\n",
    "    data = np.column_stack([feature1, feature2])\n",
    "    cov = np.cov(data.T)\n",
    "    eigenvalues = np.linalg.eigvalsh(cov)\n",
    "    condition = eigenvalues[-1] / (eigenvalues[0] + 1e-10)\n",
    "    \n",
    "    status = 'OK'\n",
    "    if actual_corr > 0.95:\n",
    "        status = 'REDUNDANT'\n",
    "    elif actual_corr > 0.7:\n",
    "        status = 'CORRELATED'\n",
    "    \n",
    "    print(f\"Target={corr_target:.2f}, Actual={actual_corr:.3f}, Condition={condition:8.2f}, Status={status}\")\n",
    "\n",
    "print(\"\\nRecommendation: Correlation < 0.7 preferred, remove if > 0.95\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Detection and Recovery Strategies\n",
    "\n",
    "### Automated Detection Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure Detector Initialized\n"
     ]
    }
   ],
   "source": [
    "class FailureDetector:\n",
    "    \"\"\"Detect regime model failures automatically.\"\"\"\n",
    "    \n",
    "    def __init__(self, thresholds=None):\n",
    "        # Default thresholds\n",
    "        self.thresholds = thresholds or {\n",
    "            'min_observations': 100,\n",
    "            'max_missing_pct': 0.05,\n",
    "            'min_confidence': 0.5,\n",
    "            'max_transitions_per_year': 50,\n",
    "            'eigenvalue_ratio': 100,\n",
    "            'condition_number': 1e6,\n",
    "            'correlation_threshold': 0.95\n",
    "        }\n",
    "    \n",
    "    def check_data_quality(self, data):\n",
    "        \"\"\"Check input data quality.\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check size\n",
    "        if len(data) < self.thresholds['min_observations']:\n",
    "            issues.append(f\"Insufficient observations: {len(data)} < {self.thresholds['min_observations']}\")\n",
    "        \n",
    "        # Check missing\n",
    "        missing_pct = data.isna().sum().sum() / (len(data) * len(data.columns))\n",
    "        if missing_pct > self.thresholds['max_missing_pct']:\n",
    "            issues.append(f\"Too much missing data: {missing_pct:.1%} > {self.thresholds['max_missing_pct']:.1%}\")\n",
    "        \n",
    "        # Check correlation\n",
    "        corr = data.corr()\n",
    "        for i in range(len(corr.columns)):\n",
    "            for j in range(i+1, len(corr.columns)):\n",
    "                if abs(corr.iloc[i, j]) > self.thresholds['correlation_threshold']:\n",
    "                    issues.append(f\"High correlation: {corr.columns[i]} <-> {corr.columns[j]} ({corr.iloc[i,j]:.3f})\")\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def check_model_health(self, model_results):\n",
    "        \"\"\"Check model training and inference health.\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check convergence\n",
    "        if not model_results.get('converged', False):\n",
    "            issues.append(\"Model did not converge (exceeded max iterations)\")\n",
    "        \n",
    "        # Check confidence\n",
    "        if 'confidence' in model_results:\n",
    "            avg_conf = model_results['confidence'].mean() if hasattr(model_results['confidence'], 'mean') else np.mean(model_results['confidence'])\n",
    "            if avg_conf < self.thresholds['min_confidence']:\n",
    "                issues.append(f\"Low average confidence: {avg_conf:.1%} < {self.thresholds['min_confidence']:.1%}\")\n",
    "        \n",
    "        # Check transitions\n",
    "        if 'predicted_state' in model_results:\n",
    "            states = model_results['predicted_state'] if hasattr(model_results['predicted_state'], 'values') else model_results['predicted_state']\n",
    "            transitions = np.sum(np.diff(states) != 0)\n",
    "            transitions_per_year = (transitions / len(states)) * 252\n",
    "            if transitions_per_year > self.thresholds['max_transitions_per_year']:\n",
    "                issues.append(f\"Too many transitions: {transitions_per_year:.0f}/year > {self.thresholds['max_transitions_per_year']:.0f}\")\n",
    "        \n",
    "        return issues\n",
    "\n",
    "# Test the detector\n",
    "detector = FailureDetector()\n",
    "print(\"Failure Detector Initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FAILURE DETECTION IN ACTION\n",
      "======================================================================\n",
      "\n",
      "Scenario 1: Good Data (500 observations)\n",
      "  ✓ All checks passed\n",
      "\n",
      "Scenario 2: Small Data (30 observations)\n",
      "\n",
      "Scenario 3: Highly Correlated Features\n"
     ]
    }
   ],
   "source": [
    "# Test detection on different data scenarios\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FAILURE DETECTION IN ACTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Scenario 1: Good data\n",
    "good_data = suite.generate_normal_data(n_obs=500)\n",
    "print(\"\\nScenario 1: Good Data (500 observations)\")\n",
    "issues = detector.check_data_quality(good_data)\n",
    "if issues:\n",
    "    for issue in issues:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"  ✓ All checks passed\")\n",
    "\n",
    "# Scenario 2: Too few observations\n",
    "small_data = suite.generate_normal_data(n_obs=30)\n",
    "print(\"\\nScenario 2: Small Data (30 observations)\")\n",
    "issues = detector.check_data_quality(small_data)\n",
    "if issues:\n",
    "    for issue in issues:\n",
    "        print(f\"  - {issue}\")\n",
    "\n",
    "# Scenario 3: High correlation\n",
    "corr_data = suite.generate_normal_data(n_obs=500)\n",
    "corr_data['feature2_dup'] = corr_data['feature1'] + np.random.normal(0, 0.01, len(corr_data))\n",
    "print(\"\\nScenario 3: Highly Correlated Features\")\n",
    "issues = detector.check_data_quality(corr_data)\n",
    "if issues:\n",
    "    for issue in issues:\n",
    "        print(f\"  - {issue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Resilience Recommendations\n",
    "\n",
    "### Layered Defense Strategy\n",
    "\n",
    "**Layer 1: Prevention**\n",
    "- Data validation before training\n",
    "- Feature correlation checking\n",
    "- Minimum sample size enforcement\n",
    "\n",
    "**Layer 2: Detection**\n",
    "- Monitor convergence during training\n",
    "- Track confidence during inference\n",
    "- Watch for regime instability\n",
    "\n",
    "**Layer 3: Recovery**\n",
    "- Fallback to univariate model\n",
    "- Use cached previous parameters\n",
    "- Reduce model complexity\n",
    "\n",
    "**Layer 4: Monitoring**\n",
    "- Daily model performance review\n",
    "- Quarterly retraining on fresh data\n",
    "- Automatic alerts on threshold violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Production Deployment Checklist\n",
    "\n",
    "### Pre-Deployment\n",
    "\n",
    "- [ ] Data quality validator implemented\n",
    "- [ ] Feature selection methodology documented\n",
    "- [ ] Thresholds set and validated\n",
    "- [ ] Fallback strategies coded\n",
    "- [ ] Unit tests pass (>60% coverage)\n",
    "- [ ] Backtest shows consistent performance\n",
    "- [ ] Stress tests pass without data snooping\n",
    "- [ ] Edge cases documented and handled\n",
    "\n",
    "### Post-Deployment\n",
    "\n",
    "- [ ] Daily monitoring dashboard active\n",
    "- [ ] Automated alerts configured\n",
    "- [ ] Logs capture all decisions and errors\n",
    "- [ ] Weekly performance review process\n",
    "- [ ] Quarterly data recalibration schedule\n",
    "- [ ] Runbook for emergency retraining\n",
    "- [ ] Model versioning for rollback capability\n",
    "\n",
    "### Monitoring Metrics\n",
    "\n",
    "| Metric | Normal | Alert | Critical |\n",
    "|--------|--------|-------|----------|\n",
    "| Average Confidence | >0.7 | 0.5-0.7 | <0.5 |\n",
    "| Transitions/Year | 20-40 | 40-60 | >60 |\n",
    "| Eigenvalue Ratio | 1-5 | 5-15 | >15 |\n",
    "| Condition Number | <1e4 | 1e4-1e6 | >1e6 |\n",
    "| Model Age | <30 days | 30-60 days | >60 days |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Failure modes are predictable and detectable**\n",
    "   - Monitor key metrics before they become problems\n",
    "   - Use automated detection framework\n",
    "\n",
    "2. **Resilience requires layered approach**\n",
    "   - Prevention > Detection > Recovery\n",
    "   - Build fallback strategies into design\n",
    "\n",
    "3. **Documentation is critical**\n",
    "   - Know what can go wrong in your use case\n",
    "   - Document thresholds and recovery procedures\n",
    "   - Train team on playbooks\n",
    "\n",
    "4. **Continuous monitoring is essential**\n",
    "   - Models degrade over time (concept drift)\n",
    "   - Quarterly retraining on fresh data\n",
    "   - Watch for regime changes in regime detector itself\n",
    "\n",
    "5. **Plan for failure**\n",
    "   - What happens if model fails completely?\n",
    "   - How long can system run on cached parameters?\n",
    "   - What's the graceful degradation path?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
