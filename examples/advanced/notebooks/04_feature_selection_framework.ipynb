{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection Framework for Multivariate HMMs\n",
    "\n",
    "**Objective:** Provide a systematic, reproducible framework for choosing which features to combine in your multivariate HMM.\n",
    "\n",
    "**Problem:** With 17 available features, which combinations work best? Is it returns + volatility? Returns + momentum? How do you decide?\n",
    "\n",
    "**Solution:** This notebook provides:\n",
    "1. Feature characteristic checklist (is a feature regime-informative?)\n",
    "2. Pre-training diagnostics (correlation, variance mismatch)\n",
    "3. Systematic testing methodology\n",
    "4. Metrics-based evaluation and ranking\n",
    "5. Decision tree for feature selection\n",
    "\n",
    "**Audience:**\n",
    "- ML engineers building regime detection systems\n",
    "- Traders/practitioners customizing for their trading strategy\n",
    "- Researchers exploring regime detection across asset classes\n",
    "\n",
    "**Expected Outcome:** A step-by-step methodology you can apply to any dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Good Features\n",
    "\n",
    "Not all features are equally useful for multivariate HMMs. A good feature must satisfy three criteria:\n",
    "\n",
    "### Criterion 1: Regime-Informative\n",
    "The feature should **vary significantly across regimes**.\n",
    "\n",
    "Example of regime-informative feature:\n",
    "- Bull regime: Realized volatility = 10% annualized\n",
    "- Bear regime: Realized volatility = 25% annualized\n",
    "- Difference: 2.5x ‚Üí Highly informative!\n",
    "\n",
    "Example of regime-ambiguous feature:\n",
    "- Bull regime: Trading volume = 50M shares\n",
    "- Bear regime: Trading volume = 55M shares\n",
    "- Difference: 1.1x ‚Üí Not very informative\n",
    "\n",
    "### Criterion 2: Non-Redundant\n",
    "The feature should **NOT provide the same information as returns**.\n",
    "\n",
    "Example of redundant features:\n",
    "- log_return and price_change ‚Üí 95% correlated (same information)\n",
    "- return_ratio and log_return ‚Üí 90% correlated (nearly identical)\n",
    "\n",
    "Example of complementary features:\n",
    "- log_return and realized_volatility ‚Üí 15% correlated (different signals)\n",
    "- log_return and momentum_strength ‚Üí 30% correlated (different signals)\n",
    "\n",
    "### Criterion 3: Scale-Stable\n",
    "The feature should **not have extreme scale differences** (handled by standardization, but matters for convergence).\n",
    "\n",
    "Example of unstable scale:\n",
    "- log_return: range [-0.05, 0.05] (small numbers)\n",
    "- raw_volume: range [10M, 100M] (large numbers)\n",
    "- Ratio: 1000x ‚Üí Numeric instability even with StandardScaler\n",
    "\n",
    "Example of stable scale:\n",
    "- log_return: range [-0.05, 0.05]\n",
    "- realized_volatility: range [0.005, 0.04]\n",
    "- Ratio: ~10x ‚Üí Manageable, pipeline handles easily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading market data (SPY 2023-2024)...\n",
      "Training on 249 observations (removed 0 NaN values), 1 feature(s)\n",
      "Downloaded 249 trading days\n",
      "\n",
      "Available observations: ['open', 'high', 'low', 'close', 'volume', 'price', 'pct_change', 'log_return', 'volatility', 'rsi']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "import hidden_regime as hr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download market data\n",
    "print(\"Downloading market data (SPY 2023-2024)...\")\n",
    "pipeline = hr.create_financial_pipeline('SPY', n_states=3, start_date='2023-01-01', end_date='2024-01-01', include_report=False)\n",
    "result = pipeline.update()\n",
    "data = pipeline.component_outputs['data']\n",
    "obs_data = pipeline.component_outputs['observations']\n",
    "\n",
    "print(f\"Downloaded {len(data)} trading days\")\n",
    "print(f\"\\nAvailable observations: {list(obs_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature Diagnostic Checklist\n",
    "\n",
    "Before training any model, run this diagnostic checklist on your candidate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE PAIR DIAGNOSTICS\n",
      "================================================================================\n",
      "\n",
      "log_return                + volatility               \n",
      "--------------------------------------------------------------------------------\n",
      "  Scale ratio: infx       (WARNING)\n",
      "  Pearson correlation: +nan\n",
      "  Spearman correlation: +nan\n",
      "  Redundancy: GOOD - Independent\n",
      "  ‚ö†Ô∏è  WARNING: Scale mismatch, but pipeline standardizes\n"
     ]
    }
   ],
   "source": [
    "def diagnose_feature_pair(feat1_name, feat2_name, data_df):\n",
    "    \"\"\"\n",
    "    Comprehensive diagnostic for a feature pair.\n",
    "    Returns dict with all diagnostic metrics.\n",
    "    \"\"\"\n",
    "    if feat1_name not in data_df.columns or feat2_name not in data_df.columns:\n",
    "        return None\n",
    "    \n",
    "    feat1 = data_df[feat1_name].dropna()\n",
    "    feat2 = data_df[feat2_name].dropna()\n",
    "    \n",
    "    # Ensure same length\n",
    "    min_len = min(len(feat1), len(feat2))\n",
    "    feat1 = feat1.iloc[:min_len]\n",
    "    feat2 = feat2.iloc[:min_len]\n",
    "    \n",
    "    diag = {\n",
    "        'feature1': feat1_name,\n",
    "        'feature2': feat2_name,\n",
    "    }\n",
    "    \n",
    "    # Scale analysis\n",
    "    scale_ratio = feat1.std() / feat2.std() if feat2.std() > 0 else np.inf\n",
    "    diag['feat1_scale'] = feat1.std()\n",
    "    diag['feat2_scale'] = feat2.std()\n",
    "    diag['scale_ratio'] = max(scale_ratio, 1/scale_ratio)\n",
    "    diag['scale_status'] = 'OK' if diag['scale_ratio'] < 100 else 'WARNING'\n",
    "    \n",
    "    # Correlation analysis\n",
    "    pearson_corr = feat1.corr(feat2)\n",
    "    spearman_corr, _ = spearmanr(feat1, feat2)\n",
    "    diag['pearson_corr'] = pearson_corr\n",
    "    diag['spearman_corr'] = spearman_corr\n",
    "    diag['abs_corr'] = abs(pearson_corr)\n",
    "    \n",
    "    if abs(pearson_corr) > 0.9:\n",
    "        diag['redundancy'] = 'REDUNDANT'\n",
    "    elif abs(pearson_corr) > 0.7:\n",
    "        diag['redundancy'] = 'Somewhat Correlated'\n",
    "    elif abs(pearson_corr) > 0.5:\n",
    "        diag['redundancy'] = 'Moderately Correlated'\n",
    "    else:\n",
    "        diag['redundancy'] = 'GOOD - Independent'\n",
    "    \n",
    "    return diag\n",
    "\n",
    "# Test key feature pairs\n",
    "feature_pairs = [\n",
    "    ('log_return', 'realized_vol'),          # Recommended\n",
    "    ('log_return', 'volatility'),            # Alternative vol measure\n",
    "    ('log_return', 'momentum_strength'),     # Alternative: momentum\n",
    "    ('log_return', 'price_change'),          # Redundant pair (should avoid)\n",
    "    ('log_return', 'directional_consistency'),  # Advanced\n",
    "    ('log_return', 'volume_ratio'),          # Volume-based\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE PAIR DIAGNOSTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "diagnostics = []\n",
    "for feat1, feat2 in feature_pairs:\n",
    "    diag = diagnose_feature_pair(feat1, feat2, obs_data)\n",
    "    if diag:\n",
    "        diagnostics.append(diag)\n",
    "        \n",
    "        print(f\"\\n{feat1:25s} + {feat2:25s}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"  Scale ratio: {diag['scale_ratio']:.1f}x       ({diag['scale_status']})\")\n",
    "        print(f\"  Pearson correlation: {diag['pearson_corr']:+.3f}\")\n",
    "        print(f\"  Spearman correlation: {diag['spearman_corr']:+.3f}\")\n",
    "        print(f\"  Redundancy: {diag['redundancy']}\")\n",
    "        \n",
    "        # Recommendation\n",
    "        if diag['redundancy'] == 'REDUNDANT':\n",
    "            print(f\"  ‚ùå AVOID: Features provide same information\")\n",
    "        elif diag['scale_ratio'] > 100:\n",
    "            print(f\"  ‚ö†Ô∏è  WARNING: Scale mismatch, but pipeline standardizes\")\n",
    "        else:\n",
    "            print(f\"  ‚úÖ GOOD: Candidate for multivariate model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Systematic Feature Testing\n",
    "\n",
    "Now let's test multiple feature combinations and rank them by quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SYSTEMATIC FEATURE COMBINATION TESTING\n",
      "================================================================================\n",
      "\n",
      "Testing: Univariate Baseline (log_return)\n",
      "Training on 249 observations (removed 0 NaN values), 1 feature(s)\n",
      "  ‚úì Converged in 100 iterations\n",
      "    Transitions: 36, Avg Confidence: 87.0%\n",
      "\n",
      "Testing: RECOMMENDED (log_return, realized_vol)\n",
      "  Feature standardization applied (variance ratio before: 14.0)\n",
      "Training on 230 observations (removed 19 NaN values), 2 feature(s)\n",
      "  ‚úì Converged in 30 iterations\n",
      "    Transitions: 11, Avg Confidence: 90.3%\n",
      "\n",
      "Testing: Alternative Vol (log_return, volatility)\n",
      "  Feature standardization applied (variance ratio before: 14.0)\n",
      "Training on 230 observations (removed 19 NaN values), 2 feature(s)\n",
      "  ‚úì Converged in 30 iterations\n",
      "    Transitions: 11, Avg Confidence: 90.3%\n",
      "\n",
      "Testing: Momentum-Focused (log_return, momentum_strength)\n",
      "  Feature standardization applied (variance ratio before: 114.5)\n",
      "Training on 229 observations (removed 20 NaN values), 2 feature(s)\n",
      "  ‚úì Converged in 100 iterations\n",
      "    Transitions: 55, Avg Confidence: 80.4%\n",
      "\n",
      "Testing: Trend-Focused (log_return, trend_persistence)\n",
      "  Feature standardization applied (variance ratio before: 3305.6)\n",
      "Training on 240 observations (removed 9 NaN values), 2 feature(s)\n",
      "  ‚úì Converged in 71 iterations\n",
      "    Transitions: 58, Avg Confidence: 91.8%\n",
      "\n",
      "================================================================================\n",
      "TEST RESULTS SUMMARY\n",
      "================================================================================\n",
      "        description  converged  transitions  avg_confidence  min_confidence\n",
      "Univariate Baseline      False           36        0.870489        0.380478\n",
      "        RECOMMENDED       True           11        0.902550        0.000000\n",
      "    Alternative Vol       True           11        0.902553        0.000000\n",
      "   Momentum-Focused      False           55        0.804377        0.000000\n",
      "      Trend-Focused       True           58        0.917696        0.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SYSTEMATIC FEATURE COMBINATION TESTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Feature combinations to test\n",
    "test_combinations = [\n",
    "    (['log_return'], 'Univariate Baseline'),\n",
    "    (['log_return', 'realized_vol'], 'RECOMMENDED'),\n",
    "    (['log_return', 'volatility'], 'Alternative Vol'),\n",
    "    (['log_return', 'momentum_strength'], 'Momentum-Focused'),\n",
    "    (['log_return', 'trend_persistence'], 'Trend-Focused'),\n",
    "]\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for features, description in test_combinations:\n",
    "    print(f\"\\nTesting: {description} ({', '.join(features)})\")\n",
    "    \n",
    "    try:\n",
    "        if len(features) == 1:\n",
    "            # Univariate\n",
    "            pipeline = hr.create_financial_pipeline(\n",
    "                'SPY',\n",
    "                n_states=3,\n",
    "                start_date='2023-01-01',\n",
    "                end_date='2024-01-01',\n",
    "                include_report=False,\n",
    "                observation_config_overrides={'generators': features}\n",
    "            )\n",
    "        else:\n",
    "            # Multivariate\n",
    "            pipeline = hr.create_multivariate_pipeline(\n",
    "                'SPY',\n",
    "                n_states=3,\n",
    "                features=features,\n",
    "                start_date='2023-01-01',\n",
    "                end_date='2024-01-01'\n",
    "            )\n",
    "        \n",
    "        report = pipeline.update()\n",
    "        result = pipeline.component_outputs['interpreter']\n",
    "        model = pipeline.model\n",
    "        \n",
    "        # Extract metrics\n",
    "        transitions = np.sum(np.diff(result['predicted_state']) != 0)\n",
    "        avg_conf = result['confidence'].mean()\n",
    "        min_conf = result['confidence'].min()\n",
    "        converged = model.training_history_['converged']\n",
    "        iterations = model.training_history_['iterations']\n",
    "        \n",
    "        test_results.append({\n",
    "            'description': description,\n",
    "            'features': ', '.join(features),\n",
    "            'converged': converged,\n",
    "            'iterations': iterations,\n",
    "            'transitions': transitions,\n",
    "            'avg_confidence': avg_conf,\n",
    "            'min_confidence': min_conf,\n",
    "            'result': result\n",
    "        })\n",
    "        \n",
    "        print(f\"  ‚úì Converged in {iterations} iterations\")\n",
    "        print(f\"    Transitions: {transitions}, Avg Confidence: {avg_conf:.1%}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Failed: {str(e)[:60]}...\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(test_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df[['description', 'converged', 'transitions', 'avg_confidence', 'min_confidence']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluation Metrics\n",
    "\n",
    "Now let's define metrics to objectively rank feature combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUALITY RANKING (Higher is Better):\n",
      "================================================================================\n",
      "ü•á #1. Alternative Vol           Score:   92.7/100\n",
      "     Features: log_return, volatility\n",
      "     Transitions: 11, Confidence: 90.3%\n",
      "\n",
      "ü•à #2. RECOMMENDED               Score:   92.7/100\n",
      "     Features: log_return, realized_vol\n",
      "     Transitions: 11, Confidence: 90.3%\n",
      "\n",
      "ü•â #3. Trend-Focused             Score:   74.3/100\n",
      "     Features: log_return, trend_persistence\n",
      "     Transitions: 58, Confidence: 91.8%\n",
      "\n",
      "   #4. Univariate Baseline       Score:   66.7/100\n",
      "     Features: log_return\n",
      "     Transitions: 36, Confidence: 87.0%\n",
      "\n",
      "   #5. Momentum-Focused          Score:   57.1/100\n",
      "     Features: log_return, momentum_strength\n",
      "     Transitions: 55, Confidence: 80.4%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_quality_score(result_row):\n",
    "    \"\"\"\n",
    "    Compute an objective quality score (0-100) for a feature combination.\n",
    "    \n",
    "    Metric weights:\n",
    "    - Convergence (30%): Did it converge quickly?\n",
    "    - Stability (40%): Fewer transitions = more stable\n",
    "    - Confidence (30%): Higher confidence = better predictions\n",
    "    \"\"\"\n",
    "    # Convergence score (100 if converged, 50 if not)\n",
    "    conv_score = 100 if result_row['converged'] else 50\n",
    "    \n",
    "    # Stability score (fewer transitions = higher score)\n",
    "    # Range: ~0-100 transitions, map to 0-100 score\n",
    "    stability_score = max(0, 100 - result_row['transitions'])\n",
    "    \n",
    "    # Confidence score (0-100% confidence, map to 0-100)\n",
    "    conf_score = result_row['avg_confidence'] * 100\n",
    "    \n",
    "    # Weighted average\n",
    "    quality = (conv_score * 0.3) + (stability_score * 0.4) + (conf_score * 0.3)\n",
    "    return quality\n",
    "\n",
    "# Compute quality scores\n",
    "if test_results:\n",
    "    results_df['quality_score'] = results_df.apply(compute_quality_score, axis=1)\n",
    "    results_df = results_df.sort_values('quality_score', ascending=False)\n",
    "    \n",
    "    print(\"\\nQUALITY RANKING (Higher is Better):\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for idx, row in results_df.iterrows():\n",
    "        rank = results_df.index.get_loc(idx) + 1\n",
    "        score = row['quality_score']\n",
    "        medal = 'ü•á' if rank == 1 else 'ü•à' if rank == 2 else 'ü•â' if rank == 3 else f'  '\n",
    "        \n",
    "        print(f\"{medal} #{rank}. {row['description']:25s} Score: {score:6.1f}/100\")\n",
    "        print(f\"     Features: {row['features']}\")\n",
    "        print(f\"     Transitions: {row['transitions']}, Confidence: {row['avg_confidence']:.1%}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Decision Tree for Feature Selection\n",
    "\n",
    "Use this interactive decision tree to choose features for YOUR specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE SELECTION DECISION TREE\n",
      "================================================================================\n",
      "\n",
      "START HERE: Do you have 2+ years of daily data?\n",
      "‚îú‚îÄ NO  ‚Üí Use UNIVARIATE (returns only)\n",
      "‚îÇ       Reason: Multivariate needs sufficient data for covariance estimation\n",
      "‚îÇ\n",
      "‚îî‚îÄ YES ‚Üí What is your primary objective?\n",
      "         ‚îú‚îÄ VOLATILITY REGIME DETECTION (High/Medium/Low Vol)\n",
      "         ‚îÇ  ‚îî‚îÄ USE: log_return + realized_vol ‚Üê RECOMMENDED\n",
      "         ‚îÇ     Why: Volatility clearly varies by regime\n",
      "         ‚îÇ          Information-theoretically optimal (see notebook 03)\n",
      "         ‚îÇ\n",
      "         ‚îú‚îÄ TREND/MOMENTUM DETECTION\n",
      "         ‚îÇ  ‚îú‚îÄ Detect momentum reversals?\n",
      "         ‚îÇ  ‚îÇ  ‚îî‚îÄ USE: log_return + momentum_strength\n",
      "         ‚îÇ  ‚îî‚îÄ Detect trend changes?\n",
      "         ‚îÇ     ‚îî‚îÄ USE: log_return + trend_persistence\n",
      "         ‚îÇ\n",
      "         ‚îú‚îÄ CRISIS DETECTION\n",
      "         ‚îÇ  ‚îî‚îÄ USE: log_return + realized_vol\n",
      "         ‚îÇ     Why: Vol spikes are immediate crisis indicators\n",
      "         ‚îÇ\n",
      "         ‚îî‚îÄ GENERAL PURPOSE (not sure)\n",
      "            ‚îî‚îÄ USE: log_return + realized_vol (DEFAULT BEST)\n",
      "               Why: Works well across most market conditions\n",
      "\n",
      "BEFORE COMMITTING, CHECK:\n",
      "1. Correlation between your chosen features (should be < 0.7)\n",
      "   ‚îî‚îÄ Use diagnose_feature_pair() above\n",
      "\n",
      "2. Scale ratio between features (should be < 100x)\n",
      "   ‚îî‚îÄ Pipeline standardizes automatically, but very large ratios may struggle\n",
      "\n",
      "3. Data quality\n",
      "   ‚îî‚îÄ No more than 3% missing data\n",
      "   ‚îî‚îÄ No obvious data errors or discontinuities\n",
      "\n",
      "4. Regime separation\n",
      "   ‚îî‚îÄ Can you visually see regimes in your features?\n",
      "   ‚îî‚îÄ Or are all observations bunched together?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def feature_selection_decision_tree():\n",
    "    \"\"\"\n",
    "    Interactive decision tree for feature selection.\n",
    "    In practice, user would answer these questions.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FEATURE SELECTION DECISION TREE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\"\"\n",
    "START HERE: Do you have 2+ years of daily data?\n",
    "‚îú‚îÄ NO  ‚Üí Use UNIVARIATE (returns only)\n",
    "‚îÇ       Reason: Multivariate needs sufficient data for covariance estimation\n",
    "‚îÇ\n",
    "‚îî‚îÄ YES ‚Üí What is your primary objective?\n",
    "         ‚îú‚îÄ VOLATILITY REGIME DETECTION (High/Medium/Low Vol)\n",
    "         ‚îÇ  ‚îî‚îÄ USE: log_return + realized_vol ‚Üê RECOMMENDED\n",
    "         ‚îÇ     Why: Volatility clearly varies by regime\n",
    "         ‚îÇ          Information-theoretically optimal (see notebook 03)\n",
    "         ‚îÇ\n",
    "         ‚îú‚îÄ TREND/MOMENTUM DETECTION\n",
    "         ‚îÇ  ‚îú‚îÄ Detect momentum reversals?\n",
    "         ‚îÇ  ‚îÇ  ‚îî‚îÄ USE: log_return + momentum_strength\n",
    "         ‚îÇ  ‚îî‚îÄ Detect trend changes?\n",
    "         ‚îÇ     ‚îî‚îÄ USE: log_return + trend_persistence\n",
    "         ‚îÇ\n",
    "         ‚îú‚îÄ CRISIS DETECTION\n",
    "         ‚îÇ  ‚îî‚îÄ USE: log_return + realized_vol\n",
    "         ‚îÇ     Why: Vol spikes are immediate crisis indicators\n",
    "         ‚îÇ\n",
    "         ‚îî‚îÄ GENERAL PURPOSE (not sure)\n",
    "            ‚îî‚îÄ USE: log_return + realized_vol (DEFAULT BEST)\n",
    "               Why: Works well across most market conditions\n",
    "\n",
    "BEFORE COMMITTING, CHECK:\n",
    "1. Correlation between your chosen features (should be < 0.7)\n",
    "   ‚îî‚îÄ Use diagnose_feature_pair() above\n",
    "\n",
    "2. Scale ratio between features (should be < 100x)\n",
    "   ‚îî‚îÄ Pipeline standardizes automatically, but very large ratios may struggle\n",
    "\n",
    "3. Data quality\n",
    "   ‚îî‚îÄ No more than 3% missing data\n",
    "   ‚îî‚îÄ No obvious data errors or discontinuities\n",
    "\n",
    "4. Regime separation\n",
    "   ‚îî‚îÄ Can you visually see regimes in your features?\n",
    "   ‚îî‚îÄ Or are all observations bunched together?\n",
    "    \"\"\")\n",
    "\n",
    "feature_selection_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Common Pitfalls & How to Avoid Them\n",
    "\n",
    "Learn from common mistakes when selecting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMMON PITFALLS IN FEATURE SELECTION\n",
      "================================================================================\n",
      "\n",
      "1. PITFALL: Using redundant features\n",
      "   Example: returns + price_change (95% correlated)\n",
      "   Problem: Provides no new information, wastes model capacity\n",
      "   Fix: Check correlation, use diagnose_feature_pair()\n",
      "\n",
      "2. PITFALL: Choosing regime-ambiguous features\n",
      "   Example: Raw trading volume (changes with time-of-day, not regime)\n",
      "   Problem: Model can't learn regime structure\n",
      "   Fix: Use volume_ratio (relative to average) not raw volume\n",
      "\n",
      "3. PITFALL: Ignoring scale differences\n",
      "   Example: returns (-0.05 to 0.05) + volume (1M to 100M)\n",
      "   Problem: Numerical instability, poor convergence\n",
      "   Fix: Pipeline standardizes, but avoid 1000x differences\n",
      "\n",
      "4. PITFALL: Using \"interesting\" but irrelevant features\n",
      "   Example: Oil prices for equity regime detection\n",
      "   Problem: No regime connection, noise only\n",
      "   Fix: Ensure feature varies significantly across YOUR regimes\n",
      "\n",
      "5. PITFALL: Too many features\n",
      "   Example: 5+ features with 2 years data\n",
      "   Problem: Curse of dimensionality, covariance matrix is near-singular\n",
      "   Fix: Start with 2 features, add a 3rd only if justified\n",
      "\n",
      "6. PITFALL: Over-fitting to historical data\n",
      "   Example: Features that worked 2018-2020 fail 2023-2024\n",
      "   Problem: Regime structure changed (market regime shift)\n",
      "   Fix: Test on multiple historical periods\n",
      "\n",
      "BEST PRACTICE:\n",
      "Always validate feature choices on out-of-sample data:\n",
      "‚îú‚îÄ Train on period A (pre-crisis or stable)\n",
      "‚îú‚îÄ Test on period B (crisis or regime shift)\n",
      "‚îî‚îÄ Ensure features still separate regimes in new period\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMMON PITFALLS IN FEATURE SELECTION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. PITFALL: Using redundant features\n",
    "   Example: returns + price_change (95% correlated)\n",
    "   Problem: Provides no new information, wastes model capacity\n",
    "   Fix: Check correlation, use diagnose_feature_pair()\n",
    "\n",
    "2. PITFALL: Choosing regime-ambiguous features\n",
    "   Example: Raw trading volume (changes with time-of-day, not regime)\n",
    "   Problem: Model can't learn regime structure\n",
    "   Fix: Use volume_ratio (relative to average) not raw volume\n",
    "\n",
    "3. PITFALL: Ignoring scale differences\n",
    "   Example: returns (-0.05 to 0.05) + volume (1M to 100M)\n",
    "   Problem: Numerical instability, poor convergence\n",
    "   Fix: Pipeline standardizes, but avoid 1000x differences\n",
    "\n",
    "4. PITFALL: Using \"interesting\" but irrelevant features\n",
    "   Example: Oil prices for equity regime detection\n",
    "   Problem: No regime connection, noise only\n",
    "   Fix: Ensure feature varies significantly across YOUR regimes\n",
    "\n",
    "5. PITFALL: Too many features\n",
    "   Example: 5+ features with 2 years data\n",
    "   Problem: Curse of dimensionality, covariance matrix is near-singular\n",
    "   Fix: Start with 2 features, add a 3rd only if justified\n",
    "\n",
    "6. PITFALL: Over-fitting to historical data\n",
    "   Example: Features that worked 2018-2020 fail 2023-2024\n",
    "   Problem: Regime structure changed (market regime shift)\n",
    "   Fix: Test on multiple historical periods\n",
    "\n",
    "BEST PRACTICE:\n",
    "Always validate feature choices on out-of-sample data:\n",
    "‚îú‚îÄ Train on period A (pre-crisis or stable)\n",
    "‚îú‚îÄ Test on period B (crisis or regime shift)\n",
    "‚îî‚îÄ Ensure features still separate regimes in new period\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Summary & Next Steps\n",
    "\n",
    "You now have a systematic framework for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE SELECTION FRAMEWORK - SUMMARY\n",
      "================================================================================\n",
      "\n",
      "STEP-BY-STEP PROCESS:\n",
      "\n",
      "1. CHARACTERIZE YOUR DATA\n",
      "   ‚úì Do you have 2+ years of daily data?\n",
      "   ‚úì What asset class? (equities/bonds/crypto/forex?)\n",
      "   ‚úì What regimes do you want to detect?\n",
      "\n",
      "2. IDENTIFY CANDIDATE FEATURES\n",
      "   Use the decision tree above to narrow down candidates\n",
      "   Typical good choices:\n",
      "   - Volatility regimes: log_return + realized_vol\n",
      "   - Momentum regimes: log_return + momentum_strength\n",
      "   - Trend regimes: log_return + trend_persistence\n",
      "\n",
      "3. RUN DIAGNOSTICS\n",
      "   Use diagnose_feature_pair() to check:\n",
      "   ‚úì Correlation (should be < 0.7)\n",
      "   ‚úì Scale ratio (should be < 100x)\n",
      "   ‚úì Are features independent?\n",
      "\n",
      "4. TEST ON HISTORICAL DATA\n",
      "   Train multivariate model:\n",
      "   ‚úì Pre-event period (stable)\n",
      "   ‚úì Check: Convergence, Transitions, Confidence\n",
      "   ‚úì Benchmark: Compare to univariate baseline\n",
      "\n",
      "5. VALIDATE ON NEW PERIOD\n",
      "   Test on different regime (crisis, downturn, etc):\n",
      "   ‚úì Do regime boundaries still make sense?\n",
      "   ‚úì Is confidence still high?\n",
      "   ‚úì Are feature distributions still regime-informative?\n",
      "\n",
      "6. DEPLOY WITH CONFIDENCE\n",
      "   Once validated:\n",
      "   ‚úì Use hr.create_multivariate_pipeline() with your chosen features\n",
      "   ‚úì Monitor performance over time\n",
      "   ‚úì Retrain if market regime fundamentally changes\n",
      "\n",
      "RESEARCH QUESTION:\n",
      "Want to test a novel feature combination? Use example 03\n",
      "to systematically compare your ideas against best practices.\n",
      "\n",
      "SEE ALSO:\n",
      "- Example 02: COVID-2020 crisis detection with features\n",
      "- Example 03: Side-by-side feature comparison\n",
      "- Notebook 03: Why volatility matters (information theory)\n",
      "- Notebook 05: How covariance structure reveals regimes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE SELECTION FRAMEWORK - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "STEP-BY-STEP PROCESS:\n",
    "\n",
    "1. CHARACTERIZE YOUR DATA\n",
    "   ‚úì Do you have 2+ years of daily data?\n",
    "   ‚úì What asset class? (equities/bonds/crypto/forex?)\n",
    "   ‚úì What regimes do you want to detect?\n",
    "\n",
    "2. IDENTIFY CANDIDATE FEATURES\n",
    "   Use the decision tree above to narrow down candidates\n",
    "   Typical good choices:\n",
    "   - Volatility regimes: log_return + realized_vol\n",
    "   - Momentum regimes: log_return + momentum_strength\n",
    "   - Trend regimes: log_return + trend_persistence\n",
    "\n",
    "3. RUN DIAGNOSTICS\n",
    "   Use diagnose_feature_pair() to check:\n",
    "   ‚úì Correlation (should be < 0.7)\n",
    "   ‚úì Scale ratio (should be < 100x)\n",
    "   ‚úì Are features independent?\n",
    "\n",
    "4. TEST ON HISTORICAL DATA\n",
    "   Train multivariate model:\n",
    "   ‚úì Pre-event period (stable)\n",
    "   ‚úì Check: Convergence, Transitions, Confidence\n",
    "   ‚úì Benchmark: Compare to univariate baseline\n",
    "\n",
    "5. VALIDATE ON NEW PERIOD\n",
    "   Test on different regime (crisis, downturn, etc):\n",
    "   ‚úì Do regime boundaries still make sense?\n",
    "   ‚úì Is confidence still high?\n",
    "   ‚úì Are feature distributions still regime-informative?\n",
    "\n",
    "6. DEPLOY WITH CONFIDENCE\n",
    "   Once validated:\n",
    "   ‚úì Use hr.create_multivariate_pipeline() with your chosen features\n",
    "   ‚úì Monitor performance over time\n",
    "   ‚úì Retrain if market regime fundamentally changes\n",
    "\n",
    "RESEARCH QUESTION:\n",
    "Want to test a novel feature combination? Use example 03\n",
    "to systematically compare your ideas against best practices.\n",
    "\n",
    "SEE ALSO:\n",
    "- Example 02: COVID-2020 crisis detection with features\n",
    "- Example 03: Side-by-side feature comparison\n",
    "- Notebook 03: Why volatility matters (information theory)\n",
    "- Notebook 05: How covariance structure reveals regimes\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
